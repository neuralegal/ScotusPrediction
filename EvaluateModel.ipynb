{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To import LSTM predictions to Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads and concatenates validation sets. Files that were produced from LSTM training may need to be renamed.\n",
    "import os\n",
    "import pandas as pd\n",
    "path = \".\\Dataframes\"\n",
    "\n",
    "df = pd.DataFrame() \n",
    "for i in range(10):\n",
    "    df_i = pd.read_csv(f\"0_KKS.csv_{i}.csv\", header = None)\n",
    "    df = df.append(df_i)\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "colnames = [\"pred_outcome\",\"outcome\",\"accuracy\",\"text\",\"docket\",\"newpred\"]\n",
    "df.columns = colnames\n",
    "df = df.drop(columns=[\"newpred\"])\n",
    "df.to_csv(f\"{path}\\\\LSTM predictions.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits DF into petitioner and respondent, checks reports accuracy for each\n",
    "def split_pet_resp(df):\n",
    "    #Splits DF into one for petitioner and one for respondent\n",
    "    respondent_rows = []\n",
    "    petitioner_rows = []\n",
    "    for i, row in df.iterrows():\n",
    "        if i%2 != 0: #Get only respondent dockets\n",
    "            respondent_rows.append(i)\n",
    "        else:\n",
    "            petitioner_rows.append(i)\n",
    "    pet_df = df.iloc[petitioner_rows]\n",
    "    rep_df = df.iloc[respondent_rows]\n",
    "    \n",
    "    return pet_df,rep_df\n",
    "\n",
    "pet_df,rep_df = split_pet_resp(df)\n",
    "print(check_accuracy(df,\"outcome\",\"pred_outcome\"))\n",
    "print(check_accuracy(rep_df,\"outcome\",\"pred_outcome\"))\n",
    "print(check_accuracy(pet_df,\"outcome\",\"pred_outcome\"))\n",
    "\n",
    "### ADD SOFTMAX PREDS TO KKS MODEL ###\n",
    "kks_df = pd.read_csv(f\"{path}\\\\feature_table.csv\")\n",
    "kks_df[\"neuralegal_resp\"] = \"\"\n",
    "kks_df[\"neuralegal_pet\"] = \"\"\n",
    "\n",
    "for i, row in kks_df.iterrows():\n",
    "    for j, rowj in rep_df.iterrows():\n",
    "        if row[\"docket\"] == rowj[\"docket\"]:            \n",
    "            pred = rowj[\"accuracy\"]\n",
    "            kks_df.at[i,\"neuralegal_resp\"] = pred\n",
    "    for k, rowk in pet_df.iterrows():\n",
    "        if row[\"docket\"] == rowk[\"docket\"]:            \n",
    "            pred = rowk[\"accuracy\"]\n",
    "            kks_df.at[i,\"neuralegal_pet\"] = pred\n",
    "\n",
    "kks_df.to_csv(f\"{path}\\\\feature_table.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check AUC on predictions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7060998151571165\n",
      "AUC Score: 0.7060998151571164\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.72      0.71       541\n",
      "           1       0.71      0.69      0.70       541\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1082\n",
      "   macro avg       0.71      0.71      0.71      1082\n",
      "weighted avg       0.71      0.71      0.71      1082\n",
      "\n",
      "Accuracy: 0.7690875232774674\n",
      "AUC Score: 0.6981832997795034\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.50      0.58       171\n",
      "           1       0.79      0.89      0.84       366\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       537\n",
      "   macro avg       0.74      0.70      0.71       537\n",
      "weighted avg       0.76      0.77      0.76       537\n",
      "\n",
      "Accuracy: 0.7388809182209469\n",
      "AUC Score: 0.6753034547152195\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.47      0.55       238\n",
      "           1       0.76      0.88      0.82       459\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       697\n",
      "   macro avg       0.71      0.68      0.68       697\n",
      "weighted avg       0.73      0.74      0.73       697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "path = \".\\Dataframes\"\n",
    "\n",
    "lstm_df = pd.read_csv(f\"{path}\\\\LSTM predictions.csv\")\n",
    "ensemble_df = pd.read_csv(f\"{path}\\\\ensemble predictions.csv\")\n",
    "KKS_df = pd.read_csv(f\"{path}\\\\results_kks.csv\")\n",
    "\n",
    "def check_accuracy(df,correctrow,predictedrow):\n",
    "    correct = 0\n",
    "    for i, row in df.iterrows():\n",
    "        if row[correctrow] == row[predictedrow]:\n",
    "            correct +=1\n",
    "    accuracy = correct/len(df)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def get_AUC(df,outcome_col,pred_col):\n",
    "    y = []\n",
    "    scores = []\n",
    "    for i, row in df.iterrows():\n",
    "        actual = row[outcome_col]\n",
    "        score = row[pred_col]\n",
    "        y.append(actual)\n",
    "        scores.append(score)\n",
    "    \n",
    "    accuracy = check_accuracy(df,outcome_col,pred_col)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"AUC Score:\", metrics.roc_auc_score(y, scores))\n",
    "    print(\"Classification Report:\\n\", metrics.classification_report(y,scores))\n",
    "\n",
    "    return\n",
    "\n",
    "get_AUC(lstm_df,\"outcome\",\"pred_outcome\")\n",
    "get_AUC(ensemble_df,\"Actual\",\"Predicted\")\n",
    "get_AUC(KKS_df,\"Actual\",\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
